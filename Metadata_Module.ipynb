{
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sail] *",
   "language": "python",
   "name": "conda-env-sail-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "timestamp": "2020-08-02T16:06:44.033133-07:00"
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata Module\n",
    "\n",
    "**This is a literate notebook.**\n",
    "\n",
    "## Motivation\n",
    "\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/Metadata)\n",
    "> Metadata is \"data that provides information about other data\". In other words, it is \"data about data.\"\n",
    "\n",
    "What were the conditions on a particular day?  The crew?  What sort of jib settings did we use?  The finishing position?  What were the shroud settings?  How did they perform?  \n",
    "\n",
    "I started out by writing a email for each race, trying to including learnings, conditions, results.  I moved to creating a Google doc for each race, easier to edit and update. And then I moved to creating a Jupyter notebook for each race day, easier to include data from the actual race all in one place.\n",
    "\n",
    "The problems with these approaches:\n",
    "\n",
    "- Repeated work.  Each email/gdoc/notebook is a vague copy of the previous, updated with new info.  This copy/edit process is annoying.\n",
    "  - For example, one step is to grab the weather/tides, and just this step takes a while by hand.\n",
    "- The data is locked in a human readable document, not in a machine readable representation.\n",
    "  - No easy way to generate a single document (i.e. table of contents that shows all races, dates and times).\n",
    "- No way to analyze the data in one place.  Where can we look to see trends or issues that are inconsistent?\n",
    "\n",
    "Philosophically, I like metadata which can be searched and cross-referenced.  Data should be easy to edit and update and view.\n",
    "\n",
    "The solution is to store all this metadata in a single easy to edit datastructure which can then be analyzed/created/edited/rendered for various needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Code to process race metadata and associate with race logs.\n",
    "\n",
    "There are currently 3 sources of hand entered metadata, hopefully fewer soon:\n",
    "- A file called metadata.yaml (in YAML).  This is the final ultimate source for metadata.\n",
    "  - YAML is super powerful, allowing you to enter complex structured info.\n",
    "  - It is also designed to be human readable (unlike a database file, or even a CSV).\n",
    "  - There is no strong schema, and its a bit disorganized.\n",
    "- A google spreadsheet (gsheet) that is fed by a [Google Form](https://forms.gle/JENZZdSWKNuoF8icA) (which is easy to use on the ride home from the race).\n",
    "  - The form determines the schema, which can be changed, but it does enforce some structure.\n",
    "  - The spreadsheet can be downloaded as a CSV from a URL.\n",
    "- An older pandas dataframe, `log_info.pd` which is should be deprecated.\n",
    "\n",
    "And a final source, which is a default and empty metadata record generated when we first upload the file.\n",
    "\n",
    "In all cases above the **key** is the date (and we therefore assume that there is a single \"file\" per day).  In practice we may have several races on a single day, though these will be in one file. The YAML file will support the ability to discuss the segments.  The Google form does not.\n",
    "\n",
    "**How to merge duplicates?**. \n",
    "\n",
    "- Multiple rows with same date in the gsheet.\n",
    "  - Delete by hand?  Take the latest?\n",
    "- Same date in gsheet and YAML.\n",
    "  - Note, gsheet row will move to YAML when it first appears.\n",
    "  - Figure out which is newer.  \n",
    "    - If YAML is newer, keep it.\n",
    "    - If gsheet is newer, then keep **both**.  Warn user and ask to edit.\n",
    "\n",
    "  \n",
    "  \n",
    "\n",
    "### References \n",
    "\n",
    "- Good reference to start: [YAML tutorial](https://rollout.io/blog/yaml-tutorial-everything-you-need-get-started/)\n",
    "  - [The official reference](https://yaml.org/) Its written in YAML (which makes it a bit weird).\n",
    "- Nice Google page on [how to use Google Forms](https://zapier.com/learn/google-sheets/how-to-use-google-forms/)\n",
    "\n",
    "\n",
    "## TODO\n",
    "\n",
    "- Add timestamp to all YAML entries?  \n",
    "\n",
    "- Tides? \n",
    "\n",
    "- Currents?  \n",
    "\n",
    "- Weather: wind, etc.  Weather buoy?\n",
    "\n",
    "- Create a page before the race? \n",
    "\n",
    "- Phone images captured during the race \n",
    "  - pull them in, link them to the map?\n",
    "  - extract settings?\n",
    "  \n",
    "- Sometimes there are two files from the same day.  In general should not have happened...  but it screws things up.\n",
    "\n",
    "- Provide a tool to slice a days data into \"segments\".\n",
    "\n",
    "\n",
    "\n",
    "## Caveats and concerns\n",
    "\n",
    "- YAML, as edited by a human author, does not support a strong schema.  Its easy to mess things up, with typos, missing fields, incorrectly named fields, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import copy\n",
    "import numbers\n",
    "import re\n",
    "\n",
    "import yaml  # We'll use YAML for our metadata\n",
    "import arrow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# These are libraries written for RegattaAnalysis\n",
    "from global_variables import G  # global variables\n",
    "import utils\n",
    "from utils import DictClass\n",
    "import process as p\n",
    "import nbutils\n",
    "from nbutils import display_markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook - an example\n",
    "\n",
    "# Below is snippet of YAML inline.  I am not going to document YAML here.  But notice that the structure\n",
    "# is reminiscent of Python itself, and it is somewhat readable.\n",
    "\n",
    "example = \"\"\"\n",
    "file: 2020-04-16_14:54.pd.gz\n",
    "date: \"2020-04-16\"\n",
    "title: Tune-up with Creative\n",
    "purpose: tune_up\n",
    "conditions: >-\n",
    "  Beautiful day. Winds were 3 quickly building to 10ish. Flat\n",
    "  seas. Upwind to Pt. Wells buoy, raised and raced home to the hamburger.\n",
    "performance: >-\n",
    "  Good height and speed vs. Creative on the way upwind. Perhaps a bit\n",
    "  slow at first downwind, exploring to tradeoffs between depth and\n",
    "  speed.  Best downwind speed when I was at the shrouds and Sara had a\n",
    "  hand on the mainsheet.\n",
    "learnings: >-\n",
    "  Let the sails out for downwind: both main and kite.  Stand forward\n",
    "  if possible.\n",
    "\n",
    "  Shroud settings seemed really great, and versatile.  With only 2 on\n",
    "  the boat, we sailed very well.  These settings are the new base!\n",
    "raceqs_video: \"https://youtu.be/9a5bLeZw8EM\"\n",
    "raceqs: \"https://raceqs.com/tv-beta/tv.htm#userId=1146016&divisionId=64190&updatedAt=2020-04-17T18:05:59Z&dt=2020-04-16T15:43:47-07:00..2020-04-16T17:39:12-07:00&boat=Creative\"\n",
    "segments:\n",
    "  - winds: [6, 12]\n",
    "    tensions: [29, 10, 0]\n",
    "    port: [2.251, 1.953, 999]\n",
    "    stbd: [2.271, 1.959, 999]\n",
    "    thoughts: >-\n",
    "      Overall we have had trouble with the Quantum quick tune card,\n",
    "      where the uppers are a bit too loose or the middles too tight.\n",
    "      The result is that the mast falls off at the top, rather than\n",
    "      staying straight or sagging for power.  We took a bit off the\n",
    "      middle (12 down to 10).\n",
    "questions:\n",
    "  - text: Was the prop set correctly?\n",
    "    author: sara\n",
    "    context: Were we slower on one tack than the other? \n",
    "    proposed_solution: ??\n",
    "\"\"\"\n",
    "\n",
    "# The data can be trivialy read in this way.  A simple Python datastructure results, in this case\n",
    "# a dictionary.  \n",
    "race_metadata = yaml.load(example, Loader=yaml.Loader)\n",
    "\n",
    "display_markdown(\"Notice that `segments` and `questions` are sublist of dicts.\")\n",
    "display(race_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Render this data to make it more readable\n",
    "\n",
    "We can prettily easily write code that \"renders\" these Python structures into readable Markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to render a metadata entry in markdown... \n",
    "\n",
    "def display_race_metadata(race_record, include_extras=True):\n",
    "    \"Summarize a race.\"\n",
    "    display_markdown(f\"# {race_record['title']}: {race_record['date']}\")\n",
    "    rr = race_record.copy()\n",
    "    for k in \"description conditions performance learnings\".split():\n",
    "        if k in rr:\n",
    "            display_section(k.capitalize(), rr.pop(k))\n",
    "    links = \"raceqs raceqs_video\".split()\n",
    "    if has_key(links, race_record):\n",
    "        display_markdown(\"## Links\")\n",
    "        lines = \"\"\n",
    "        for k in links:\n",
    "            if k in rr:\n",
    "                lines += lines_url(make_title(k), rr.pop(k))\n",
    "        display_markdown(lines)\n",
    "    if include_extras:\n",
    "        keys = list(rr.keys())\n",
    "        if len(keys) > 0:\n",
    "            lines = \"\"\n",
    "            display_markdown(\"## Extras\")\n",
    "            lines = lines_dict(\"\", keys, rr)\n",
    "            display_markdown(lines)\n",
    "\n",
    "def has_key(key_list, dictionary):\n",
    "    for k in key_list:\n",
    "        if k in dictionary:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_list_of_dicts(val):\n",
    "    return isinstance(val, list) and isinstance(val[0], dict)\n",
    "\n",
    "def lines_dict(prefix, keys, dictionary):\n",
    "    lines = \"\"\n",
    "    for k in keys:\n",
    "        val = dictionary[k]\n",
    "        if is_list_of_dicts(val):\n",
    "            for i, v in enumerate(val):\n",
    "                lines += f\"{prefix}- **{k}: {i}**\\n\"\n",
    "                lines += lines_dict(prefix+\"  \", v.keys(), v)\n",
    "        else:\n",
    "            lines += f\"{prefix}- **{k}**: {val}\\n\"\n",
    "    return lines\n",
    "            \n",
    "def display_section(title, text):\n",
    "    \"Displays a markdown section with text.\"\n",
    "    display_markdown(f\"## {title}\")\n",
    "    display_markdown(text)        \n",
    "        \n",
    "def is_url(s):\n",
    "    return s.startswith(\"http\")  # Not great, but OK for now\n",
    "\n",
    "def make_title(key):\n",
    "    \"\"\"\"\n",
    "    YAML keys are python keywords (lowercase and separated by underscores).  This converts to a pretty \n",
    "    and printable string.\n",
    "    \"\"\"\n",
    "    words = key.split(\"_\")\n",
    "    words = [w.capitalize() for w in words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "def lines_url(link_text, url):\n",
    "    \"Displays a markdown URL.\"\n",
    "    return f\"- [{link_text}]({url})\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook - Create a human readable version of this metadata\n",
    "display_race_summary(race_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Race metadata is stored as a multi-document sequence in the YAML file.  \n",
    "\n",
    "def read_metadata():\n",
    "    \"\"\"\n",
    "    Read the race metadata and return a struct, with a \n",
    "    - timestamp\n",
    "    - records:  list of records\n",
    "    - dates:    dict from date to record\n",
    "    \"\"\"\n",
    "    race_yaml = read_yaml(G.METADATA_PATH)\n",
    "    dates = {}\n",
    "    records = []\n",
    "    for record in race_yaml:\n",
    "        # If the record is missing a source, assume it was written byhand.\n",
    "        if 'source' not in record:\n",
    "            record['source'] = 'byhand'\n",
    "        dates[record['date']] = record\n",
    "        records.append(record)\n",
    "    # File timestamp, used to find valid updates in other sources.\n",
    "    ts = arrow.get(os.path.getmtime(G.METADATA_PATH)).to('US/Pacific')\n",
    "    G.logger.info(f\"Read {len(records)} records.\")\n",
    "    return DictClass(dates=dates, records=records, timestamp=ts)\n",
    "\n",
    "def save_metadata(race_records):\n",
    "    \"\"\"\n",
    "    Save a sequence of race records to the metadata file.\n",
    "    \"\"\"\n",
    "    G.logger.info(f\"Writing {len(race_records)} records.\")    \n",
    "    utils.backup_file(G.METADATA_PATH)\n",
    "    # For convenience sort the records by date before writing.\n",
    "    sorted_records = sorted(race_records, key=lambda r: r['date'])\n",
    "    with open(G.METADATA_PATH, 'w') as yfs:\n",
    "        save_yaml(sorted_records, yfs)\n",
    "\n",
    "# A bit of magic here to ensure we have the best loader/dumper.  Specifying this is required when \n",
    "# calling load/dump (below).\n",
    "try:\n",
    "    from yaml import CLoader as Loader, CDumper as Dumper\n",
    "except ImportError:\n",
    "    from yaml import Loader, Dumper\n",
    "\n",
    "def read_yaml(yaml_path):\n",
    "    \"Read the race records stored in the YAML file.  Return a dict indexed by date string: YYYY-MM-DD.\"\n",
    "    with open(yaml_path, 'r') as yaml_stream:\n",
    "        race_yaml = list(yaml.load_all(yaml_stream, Loader=Loader))\n",
    "    return race_yaml\n",
    "\n",
    "                     \n",
    "def save_yaml(race_entries, stream=None):\n",
    "    \"Save the race metadata as YAML.\"\n",
    "    return yaml.dump_all(race_entries, stream, Dumper=Dumper,\n",
    "                         default_flow_style=False, sort_keys=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook - load some metadata\n",
    "\n",
    "metadata = read_metadata()\n",
    "display(metadata.timestamp)\n",
    "\n",
    "display_race_summary(metadata.dates['2020-04-19'])\n",
    "display_race_summary(metadata.dates['2020-06-06'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the data for all days is in one place its easy to generate an all up summary.\n",
    "\n",
    "def race_summary(race):\n",
    "    display_markdown(race_summary_lines(race))\n",
    "\n",
    "def race_summary_lines(race):\n",
    "    lines = \"\"\n",
    "    lines += f\"- **{race['date']}**: {race['title']}\\n\"\n",
    "    for key in \"description conditions\".split():\n",
    "        if key in race and race[key] is not None and len(race[key]) > 0:\n",
    "            lines += f\"  - *{key.capitalize()}:* {race[key]}\\n\"\n",
    "    return lines\n",
    "\n",
    "def display_race_summaries(race_records):\n",
    "    \"Summarize each race.\"\n",
    "    lines = \"\"\n",
    "    for race in race_records:\n",
    "        lines += race_summary_lines(race)\n",
    "    display_markdown(lines)\n",
    "\n",
    "def summary_table(race_records, columns = None):\n",
    "    \"Return a summary table the races.\"\n",
    "    rows = []\n",
    "    if columns is None:\n",
    "        columns = \"date title file source\".split()\n",
    "    for race in race_records:\n",
    "        row = {k:race.get(k, '') for k in columns}\n",
    "        rows.append(row)\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook \n",
    "\n",
    "summarize(metadata.records)\n",
    "summary_table(metadata.records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook - metadata schema... light.\n",
    "\n",
    "# We currently do not have a schema for the metadata, it is implicit, which is dangerous.  We\n",
    "# can extract fields and their types.\n",
    "#\n",
    "# Note, we make a simplifying assumption that we have three situations: \n",
    "# i) primitive types, ii) dicts, ii) list of dicts. \n",
    "#\n",
    "# We do not have dicts containing dicts.\n",
    "\n",
    "def schema_lite(race_data):\n",
    "    \"Find a lightweight schema from the existing data.  Provides a guide for future entries.\"\n",
    "    fd = flatten_dicts(\"\", race_data)\n",
    "    return distill(fd)    \n",
    "\n",
    "def distill(race_union):\n",
    "    res = {}\n",
    "    for k, v in race_union.items():\n",
    "        if len(v) == 0:\n",
    "            res[k] = v[0]\n",
    "        elif isinstance(v[0], dict):\n",
    "            # print(k, \"collapsing\")\n",
    "            collapsed = collapse_dicts(v)\n",
    "            # print(k, \"distilling\", collapsed)\n",
    "            res[k] = distill(collapsed)\n",
    "            # print(k, \"done\")\n",
    "        else:\n",
    "            res[k] = set([type(e) for e in v])\n",
    "    return res\n",
    "\n",
    "def flatten_dicts(prefix, dicts):\n",
    "    \"Pass over a list of dicts and extract fields, and collect all the values assined to those fields.  For fields which are list of dicts, recurse.  Combine and flatten all fields.\"\n",
    "    res = {}\n",
    "    for d in dicts:\n",
    "        for k, v in d.items():\n",
    "            if is_list_of_dicts(v):\n",
    "                # print(k, v)\n",
    "                v = flatten_dicts(k + \"[]\", v)\n",
    "                res.update(v)\n",
    "            else:\n",
    "                res[prefix+k] = res.get(k, []) + [v]\n",
    "    return res\n",
    "\n",
    "def distill_types(race_union):\n",
    "    \"For each key in race_union return a set containing the \"\n",
    "    res = {}\n",
    "    for k, v in race_union.items():\n",
    "        res[k] = set([type(e) for e in v])\n",
    "    return res\n",
    "\n",
    "def add_key(res, key_list, val):\n",
    "    if len(key_list) > 0:\n",
    "        base_key = key_list[0]\n",
    "        if len(key_list) == 1:\n",
    "            res[base_key] = res.get(base_key, set()).union(val)\n",
    "        else:\n",
    "            next_dict = res.get(base_key, {})\n",
    "            next_dict.update(add_key)\n",
    "            if base_key in res:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The fields in the metadata schema.\n",
    "\n",
    "- The keys in this dict are the fields used.\n",
    "   - If `foo[]bar` then field `foo` is a list of records, each containing the field `bar`.\n",
    "- In both cases the value is a set of types that are encountered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook - extract the \"schema\"\n",
    "\n",
    "schema_lite(metadata.records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading metadata from Google Forms\n",
    "\n",
    "We created a Google form for post race metadata entry: https://forms.gle/JENZZdSWKNuoF8icA\n",
    "\n",
    "The advantage of this **public** form is that it is easy to enter data from any device (including mobile) at any time.  And the schema is at least \"weakly\" enforced.\n",
    "\n",
    "The form provide a scheme for publishing the resulting data as a spreadsheet, which is here: https://docs.google.com/spreadsheets/d/e/2PACX-1vS5g8oeSAMk-CFP-xDi4hu9a23W-iF5SMNjap-Gd78BPWvhA1GGgpDqFkQaEUVD3zoM9Pud1fozuDn8/pub?output=csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook \n",
    "\n",
    "URL = r\"https://docs.google.com/spreadsheets/d/e/2PACX-1vS5g8oeSAMk-CFP-xDi4hu9a23W-iF5SMNjap-Gd78BPWvhA1GGgpDqFkQaEUVD3zoM9Pud1fozuDn8/pub?output=csv\"\n",
    "\n",
    "raw_gsheet = pd.read_csv(URL)  # index_col=0, parse_dates=True)\n",
    "\n",
    "for c in raw_gsheet.columns:\n",
    "    print(repr(c))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the current list of columns used in the form.  Note the column names are long and verbose,\n",
    "# in Google Forms the column names are also the documentation for the form fields.\n",
    "\n",
    "# Long name and a convenient short form.\n",
    "SHORT_COLNAME_TO_LONG = {\n",
    "    'date'            : 'Date YYYY-MM-DD (e.g. \"2020-05-10\") or blank for today.',\n",
    "    'title'           : 'Title: short name for sail (e.g. SBYC Snowbird #1)', \n",
    "    'purpose'         : 'Purpose',\n",
    "    'crew'            : 'Crew',\n",
    "    'description'     : 'Description (2-3 sentences, optional)',\n",
    "    'conditions'      : 'Conditions (i.e. description)',\n",
    "    'performance'     : 'Performance (i.e. how did we perform vs other boats or polars).',\n",
    "    'learnings'       : \"Learnings (something we'd like to repeat or avoid)\",\n",
    "    'warnings'        : 'Warnings (needed repair, change, etc).',\n",
    "    'wave'            : 'Wave height (pick best)',\n",
    "    'wind'            : 'Winds in knots (pick best description)',\n",
    "    'port_pointing'   : 'Port Pointing',\n",
    "    'stbd_pointing'   : 'Starboard Pointing',\n",
    "    'settings'        : 'Settings Summary (how were sail controls set? trim?)',\n",
    "    'shroud_name'     : 'Shrouds (short name)',\n",
    "    'shroud_tension'  : 'Shroud tensions (UP, MID, LOW: comma sep: 29,10,0). Pos low is tension, neg low is circle size in cm).',\n",
    "    'other'           : 'Other (try to be structured!)',\n",
    "    'additional_crew' : 'Additional Crew (comma separated)',\n",
    "    'timestamp'       : 'Timestamp',\n",
    "    'fluid_comments'  : 'Comments on Fluids?',\n",
    "    'fluids'          : 'Gas, Water, Pump Out, Empty Bilge?',\n",
    "}\n",
    "\n",
    "LONG_COLNAME_TO_SHORT = {v:k for k, v in SHORT_COLNAME_TO_LONG.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gsheet():\n",
    "    \"Read the latest GSHEET.  Check that nothing bad has happened, and convert to short names.\"\n",
    "    gs = pd.read_csv(G.GSHEET_URL)\n",
    "    check_columns_changed(gs, LONG_COLNAME_TO_SHORT)\n",
    "    return convert_to_short_names(gs)\n",
    "\n",
    "def check_columns_changed(df, long_colname_map):\n",
    "    \"\"\"\n",
    "    Its entirely possible that I will someday edit the form and then the columns will get\n",
    "    out of whack.  Check to see that there are neither new fields or missing fields.\n",
    "    \"\"\"\n",
    "    new_colnames = []\n",
    "    missing_colnames = []\n",
    "    for c in df.columns:\n",
    "        if c not in long_colname_map:\n",
    "            new_colnames.append(c)\n",
    "    for c in long_colname_map:\n",
    "        if c not in df.columns:\n",
    "            missing_colnames.append(c)\n",
    "    if len(new_colnames) == 0 and len(missing_colnames) == 0:\n",
    "        G.logger.info(\"No missing or extra columns.\")\n",
    "        return True\n",
    "    else:\n",
    "        G.logger.warning(f\"Uh Oh. New cols {new_colnames}. Missing cols {missing_colnames}.\")\n",
    "        return False\n",
    "\n",
    "def convert_to_short_names(raw_metadata):\n",
    "    \"Assuming the columns names have not changed, convert to a short form.\"\n",
    "    return raw_metadata.rename(LONG_COLNAME_TO_SHORT, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook - read the sheet and display the rows\n",
    "\n",
    "gsheet = read_gsheet()\n",
    "gsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next step is to convert the Google Form spreadsheet rows to race metadata entries.  The goal is to \n",
    "# keep the two \"close\" so that conversion is not onerous.\n",
    "\n",
    "def gsheet_row_to_metadata(row):\n",
    "    \"Convert the Google Form spreadsheet rows to race metadata entries.\"\n",
    "    res = {}\n",
    "    # If both the date and the timestamp is missing then something must be wrong.\n",
    "    if is_missing_value(row['date']) and is_missing_value(row['timestamp']):\n",
    "        G.logger.info(f\"Encountered a row with missing date and timestamp. Skipping.\")\n",
    "        return None\n",
    "    for key, val in row.iteritems():\n",
    "        if key == 'date' and is_missing_value(val):\n",
    "            val = timestamp_convert(row['timestamp']).format(\"YYYY-MM-DD\")\n",
    "        if is_missing_value(val):\n",
    "            continue\n",
    "        elif key == 'timestamp':\n",
    "            val = timestamp_convert(row['timestamp']).datetime\n",
    "        elif key == 'wind':\n",
    "            val = [int(s.strip()) for s in val.split(\"-\")]\n",
    "        elif key == 'crew':\n",
    "            val = [s.strip() for s in val.split(\",\")]\n",
    "        elif key == 'shroud_tension':\n",
    "            val = [int(s.strip()) for s in val.split(\",\")]\n",
    "        res[key] = val\n",
    "    res['source'] = 'gsheet'\n",
    "    # It is a bit easier if the fields are in the same/similar order.\n",
    "    return reorder_some_keys(res, SHORT_COLNAME_TO_LONG.keys())\n",
    "\n",
    "def reorder_some_keys(dictionary, keys):\n",
    "    \"Return a dictionary with the keys in the order presented in keys.\"\n",
    "    d = copy.copy(dictionary)\n",
    "    res = dict()\n",
    "    # Copy the ones in keys\n",
    "    for k in keys:\n",
    "        if k in d:\n",
    "            res[k] = d.pop(k)\n",
    "    # Copy the rest.\n",
    "    for k in d.keys():\n",
    "        res[k] = d[k]\n",
    "    return res\n",
    "\n",
    "def is_missing_value(val):\n",
    "    \"Pandas replaces empty CSV entries with NaN.  Return True if encountered.\"\n",
    "    return isinstance(val, numbers.Number) and np.isnan(val)\n",
    "\n",
    "def timestamp_convert(val):\n",
    "    \"Convert the google forms timestamp to a date.\"\n",
    "    return arrow.get(val, 'M/D/YYYY H:mm:ss', tzinfo='US/Pacific')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook - convert from gsheet to metadata format\n",
    "\n",
    "gsheet_records = [gsheet_row_to_metadata(row) for index, row in gsheet.iterrows()]\n",
    "\n",
    "# Show one row.\n",
    "print(save_yaml(gsheet_records[-1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there are new rows in the GSHEET, then add them to metadata.\n",
    "\n",
    "def add_gsheet_records(gsheet, metadata):\n",
    "    \"Find records in the gsheet which are missing from the existing metadata.\"\n",
    "    dates = metadata.dates.copy()\n",
    "    res = []\n",
    "    for _, row in gsheet.iterrows():\n",
    "        record = gsheet_row_to_metadata(row)\n",
    "        if record is not None:\n",
    "            date = record['date']\n",
    "            G.logger.debug(f\"Examining record {date}\")\n",
    "            if date not in dates:\n",
    "                G.logger.info(f\"Found new record for {date} : {record.get('title', '')}\")\n",
    "                res.append(record)\n",
    "            else:\n",
    "                existing = dates.pop(date)\n",
    "                source = existing['source']\n",
    "                if source == 'byhand':\n",
    "                    if metadata.timestamp < record['timestamp']:\n",
    "                        G.logger.warning(f\"Duplicate record. GSheet row is newer than byhand metadata: {record['timestamp']}.\")\n",
    "                        # Append both, we'll need to figure this out by hand\n",
    "                        res.append(existing)\n",
    "                        res.append(record)\n",
    "                    else:\n",
    "                        res.append(existing)\n",
    "                elif source in ['loginfo', 'logprocess', 'gsheet']:\n",
    "                    G.logger.debug(f\"Merging gsheet into exiting record.\")\n",
    "                    # Overwrite the values in these records.\n",
    "                    new_record = {**existing, **record}\n",
    "                    res.append(new_record)\n",
    "                else:\n",
    "                    G.logger.warning(f\"Found strange source: {source}.\")\n",
    "    return res + list(dates.values())\n",
    "\n",
    "def update_metadata_from_gsheet():\n",
    "    \"Read metadata.yml and gsheet and update as needed.\"\n",
    "    metadata = read_metadata()\n",
    "    gsheet = read_gsheet()\n",
    "    new_records = add_gsheet_records(gsheet, metadata)\n",
    "    save_metadata(new_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook \n",
    "\n",
    "# loginfo is a legacy location for metadata, need to pull that in...  once.\n",
    "\n",
    "log_info = pd.read_pickle(G.LOG_INFO_PATH)\n",
    "display(len(log_info))\n",
    "log_info[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_metadata_from_loginfo():\n",
    "    \"Read metadata.yml and loginfo and update as needed.\"\n",
    "    metadata = read_metadata()\n",
    "    loginfo = pd.read_pickle(G.LOG_INFO_PATH)    \n",
    "    updated = merge_loginfo_records(loginfo, metadata)\n",
    "    save_metadata(updated)\n",
    "\n",
    "\n",
    "# Process the legacy loginfo data, this is only needed once.\n",
    "def merge_loginfo_records(loginfo, metadata):\n",
    "    \"\"\"\n",
    "    Create a metadata record for each loginfo record.  When a key already exists in\n",
    "    metadata merge the info, overwriting fields in the loginfo record.\n",
    "    \"\"\"\n",
    "    dates = metadata.dates.copy()\n",
    "    rows = []\n",
    "    for i, row in loginfo.iterrows():\n",
    "        adt = datetime_from_log_filename(row.file)\n",
    "        date_string = date_from_datetime(adt)\n",
    "        record = {}\n",
    "        # Munge loginfo data into \"metadata\" schema.\n",
    "        record['file'] = row.file\n",
    "        record['date'] = date_string\n",
    "        record['title'], record['description'] = loginfo_title(row)\n",
    "        record['begin'] = row.begin\n",
    "        record['end'] = row.end\n",
    "        record['source'] = 'loginfo'\n",
    "        # Overwrite with the existing record... if it exists.\n",
    "        if date_string in dates:\n",
    "            record.update(dates.pop(date_string))\n",
    "        rows.append(record)\n",
    "    return rows + list(dates.values())\n",
    "\n",
    "def loginfo_title(row):\n",
    "    \"Create a title and description from row record.\"\n",
    "    if len(row.race) > 0:\n",
    "        return row.race, row.description\n",
    "    else:\n",
    "        return row.description, ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, during upload we should ensure that there is a default and empty metadata record \n",
    "# for each race.\n",
    "\n",
    "def add_missing_metadata():\n",
    "    \"\"\"\n",
    "    Working backward from the full list of pandas datafiles, ensure there is a default\n",
    "    entry in the metadata file for each.\n",
    "    \"\"\"\n",
    "    metadata = read_metadata()\n",
    "    dates = metadata.dates.copy()\n",
    "    new_dates = {}\n",
    "    pfiles = p.pandas_files()\n",
    "    G.logger.info(f\"Found {len(pfiles)} pandas files.\")\n",
    "    for f in sorted(pfiles):\n",
    "        adt = datetime_from_log_filename(f)\n",
    "        date = date_from_datetime(adt)\n",
    "        G.logger.debug(f\"Examining {date} : {f}\")\n",
    "        # Default metadata... basically empty\n",
    "        record = dict(file=f, date=date, title=date, begin=0, end=-1, source='logprocess')\n",
    "        if date not in dates:\n",
    "            G.logger.info(f\"Found missing entry for {date} : {f}.\")\n",
    "        if date in new_dates:\n",
    "            G.logger.warning(f\"Two files for {date}. Watch out. Skipping.\")\n",
    "        else:\n",
    "            existing = dates.pop(date, {})\n",
    "            record.update(existing)\n",
    "            new_dates[date] = record\n",
    "    all_records = list(new_dates.values()) + list(dates.values())\n",
    "    G.logger.info(f\"Outputting {len(all_records)} records.\")\n",
    "    save_metadata(all_records)\n",
    "\n",
    "\n",
    "def datetime_from_log_filename(filename, time_zone='US/Pacific'):\n",
    "    \"Extracts the datetime from a log filename.\"\n",
    "    dt_string = re.sub(r\".gz$\", \"\", filename)   # Could be compressed\n",
    "    dt_string = re.sub(r\".pd$\", \"\", dt_string)  # Standard .pd\n",
    "    return arrow.get(dt_string, \"YYYY-MM-DD_HH:mm\", tzinfo=time_zone)\n",
    "    \n",
    "\n",
    "def date_from_datetime(adt):\n",
    "    return adt.format(\"YYYY-MM-DD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_race(updated_race_record):\n",
    "    \"Replace the race record, by date.\"\n",
    "    date = updated_race_record['date']\n",
    "    md = read_metadata()\n",
    "    if date not in md.dates:\n",
    "        raise Exception(f\"Warning, {date} could not be found in the race logs.\")\n",
    "    md.dates[date] = updated_race_record\n",
    "    save_metadata(list(md.dates.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook - \n",
    "\n",
    "if False:\n",
    "    update_metadata_from_loginfo()\n",
    "    add_missing_metadata()\n",
    "    update_metadata_from_gsheet()"
   ]
  }
 ]
}