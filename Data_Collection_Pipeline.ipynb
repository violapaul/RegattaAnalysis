{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection Pipeline\n",
    "\n",
    "**Summary**: This notebook contains a discussion of the data which is captured on boat and how that data makes its way from the boat network, to the Raspberry Pi, and then onto a laptop for analysis.  This is ultimately boring stuff (\"just plumbing\"), but it is important that this be done right or data will be lost, duplicated, or corrupted.\n",
    "\n",
    "If you are interested in the content of this data, please read [Preparing Boat Instrument Data for Analysis](Data_Preparation.ipynb).\n",
    "\n",
    "\n",
    "## Tenets\n",
    "\n",
    "Every design should have tenets.  These are high level requirements that potentially impact a large portion of the design.\n",
    "\n",
    "- Keep the original raw data. If there is a bug in the processing pipeline then the data can be re-processed.\n",
    "  - Keep at least one copy of all data.\n",
    "- Process all data, but flag useful vs useless.\n",
    "- Only copy changes, to make things fast\n",
    "- It should be easy to navigate data\n",
    "  - Filenames should make sense and be organized\n",
    "  - Additional Databases that cannot be easily hand editted are bad.  YAML?\n",
    "- The RPi is ephemeral.  Assume it can be lost!\n",
    "\n",
    "\n",
    "## TODO\n",
    "\n",
    "- Eliminate full rsync.  Might be expensive, and could compete with logging.  Not\n",
    "    needed.\n",
    "- Add a full_backup script that does do a full backup!  Rsync, etc.\n",
    "- Delete old logs from RPi.\n",
    "- Reprocess all old logs.\n",
    "\n",
    "\n",
    "## User Experience\n",
    "\n",
    "- Come home, pull out USB stick.\n",
    "- Put in Mac.\n",
    "- Run pull data script\n",
    "  - Copies all data over to Mac\n",
    "  - Finds new logs and processes them.\n",
    "\n",
    "\n",
    "## Design\n",
    "\n",
    "- Invariant: All processed logs files are compressed (saves 10x for 300 Megs).\n",
    "- Operations are idempotent where possible (you can run them again and again).\n",
    "  - Operations compute what would result, and then only trigger if needed.\n",
    "- All data kept in a raw logs directory, using RPi filenames\n",
    "  - Find the new files on the USB stick which do not have compressed versions.\n",
    "  - Copy and then compress.\n",
    "- Maintain another directory with links to useful logs, with friendly names (based on datetime and boat name).\n",
    "\n",
    "- Keep things simple. Operations are: `op file_in file_out` with an optional `force`.\n",
    "  - If `file_out` exists, then you skip the step unless `force`.\n",
    "\n",
    "- Operations:\n",
    "  - `copy_rpi_log_file(\n",
    "  - `extract_file_name(compressed_log_file)`:  Given a raw log, extract a better filename.\n",
    "    - Got to be super careful here that we are using the **first** datetime record!\n",
    "        Otherwise multiple files can be generated.\n",
    "  - `link_file(compressed_log_file, new_path)`\n",
    "  - `canboat_to_json(compressed_log_file, json_path)`\n",
    "  - `json_to_gpx(json_path, gpx_path)`\n",
    "  - `json_to_pandas(json_path, gpx_path)`\n",
    "\n",
    "- Orchestration can be done by the file, or by the directory.\n",
    "  - Process directory: find all files, and then trigger the pipeline.\n",
    "    - User will want this, because it will automaically process all new logs\n",
    "  - Or a single file can be processed, most likely with `force`.\n",
    "\n",
    "\n",
    "\n",
    "## Questions\n",
    "\n",
    "- what processing on the RPi vs. on the laptop?\n",
    "  - generally want to keep the RPi very simple... for now.\n",
    "- Some files are in the git repo, others not.  Which ones? Is this good?\n",
    "- What about the err files?\n",
    "- Do we need to clean up files?  Remove from RPi?\n",
    "  - Mostly empty files, from runs at home where there is no canbus\n",
    "  - very old files\n",
    "\n",
    "\n",
    "## Facts\n",
    "\n",
    "- Current logs go back to about October\n",
    "- Raw logs are large, up to 350 Meg, (compressed is 1/10 th)\n",
    "- Raw logs are processed into: GPX files, JSON files, and Pandas files.\n",
    "  - All are large and could be usefully compressed\n",
    "  - JSON is not useful in itself, but is more easily readable than raw.\n",
    "- We are unlikely to collect more than 10 real logs a month\n",
    "- Logging takes about 1Meg a minute.  Less than 10Meg is not useful.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Ideas\n",
    "- Does it make sense to use symlinks instead of renaming of files\n",
    "- Or just skip renaming, use the RPi log files, and use a association table.\n",
    "  - I have found this disorienting, since the filenames are pretty much random.\n",
    "\n",
    "\n",
    "- Tenets\n",
    "  - Never delete data permanently\n",
    "  - Keep the raw data, there is no telling if a bug in the pipeline \n",
    "  - Process all data, but flag useful vs useless.\n",
    "  - Only copy changes, to make things fast\n",
    "  - It should be easy to navigate data\n",
    "    - Filenames should make sense and be organized\n",
    "    - Additional DBs that cannot be easily hand editted are bad.  YAML?\n",
    "  - RPi is ephemeral.  Assume it can be lost!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sail] *",
   "language": "python",
   "name": "conda-env-sail-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
