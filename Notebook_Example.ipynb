{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Example Notebook\n",
    "\n",
    "Just a simple exmaple of a literate programming notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import JSON, its handy!\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'menu': {'id': 'file',\n",
       "  'value': 'File',\n",
       "  'popup': {'menuitem': [{'value': 'New', 'onclick': 'CreateNewDoc()'},\n",
       "    {'value': 'Open', 'onclick': 'OpenDoc()'},\n",
       "    {'value': 'Close', 'onclick': 'CloseDoc()'}]}}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# notebook\n",
    "\n",
    "# Here is a cell that will not make it into the \"module\"\n",
    "\n",
    "json_example = \"\"\"\n",
    "{\"menu\": {\n",
    "  \"id\": \"file\",\n",
    "  \"value\": \"File\",\n",
    "  \"popup\": {\n",
    "    \"menuitem\": [\n",
    "      {\"value\": \"New\", \"onclick\": \"CreateNewDoc()\"},\n",
    "      {\"value\": \"Open\", \"onclick\": \"OpenDoc()\"},\n",
    "      {\"value\": \"Close\", \"onclick\": \"CloseDoc()\"}\n",
    "    ]\n",
    "  }\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "json_dict = json.loads(json_example)\n",
    "json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is part of the module\n",
    "\n",
    "def count_keys(json_dict):\n",
    "    \"Count the number of keys in a JSON dict.\"\n",
    "    # This code is just an example... there are better ways!\n",
    "    count = 0\n",
    "    for k in json_dict:\n",
    "        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# notebook \n",
    "\n",
    "count_keys(json_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "markdown\n",
      "markdown\n",
      "code\n",
      "##########################################################\n",
      "##########################################################\n",
      "##########################################################\n",
      "# Load some libraries\n",
      "%matplotlib notebook\n",
      "from IPython.display import display, Markdown, Latex\n",
      "\n",
      "import global_variables\n",
      "G = global_variables.init_seattle()code\n",
      "##########################################################\n",
      "##########################################################\n",
      "##########################################################\n",
      "import logging\n",
      "import copy\n",
      "import itertools as it\n",
      "import datetime\n",
      "\n",
      "import numpy as np\n",
      "import scipy\n",
      "import scipy.interpolate\n",
      "import pandas as pd\n",
      "import arrow\n",
      "\n",
      "import global_variables\n",
      "G = global_variables.init_seattle()\n",
      "\n",
      "import canboat as cb\n",
      "import chart as c\n",
      "import process as p\n",
      "\n",
      "# Helpful constancts for NMEA 2K PGNs\n",
      "PGN_GNSS         = cb.pgn_code('GNSS Position Data')\n",
      "PGN_AIS_POSITION = 129039  # AIS Class B Position Report\n",
      "PGN_AIS_DATA     = 129809  # AIS Class B static data (msg 24 Part A)\n",
      "\n",
      "NMEA_SRC         = 8  # V50 radio\n",
      "\n",
      "MIN_DELTA_TIME = datetime.timedelta(microseconds = 10000) # 10 milliseconds\n",
      "\n",
      "def ais_dataframes(records):\n",
      "    \"Return two dataframes, one that records the positions of all AIS craft.  The second that records the names.\"\n",
      "    # The only thing that makes this a bit tricky is getting the correct GPS times.  The\n",
      "    # row timestamps are incorrect, so we need to wait for a GNSS record to go by, and\n",
      "    # then determines the true GPS time.\n",
      "    pgns_to_collect = {PGN_AIS_POSITION, PGN_AIS_DATA}\n",
      "    pgn_rows = {k:[] for k in pgns_to_collect}\n",
      "    delta_time = None\n",
      "    for record_num, record in zip(it.count(), records):\n",
      "        if record_num % 500000 == 0:\n",
      "            logging.info(f\"Processed {record_num} json lines.\")\n",
      "        pgn = record['pgn']\n",
      "        src = record['src']\n",
      "        if delta_time is None and pgn == PGN_GNSS:\n",
      "            # Row time (from Raspberry PI)\n",
      "            timestamp = arrow.get(record['timestamp'])\n",
      "            # GPS time, from the GNSS record.\n",
      "            gnss_datetime = cb.convert_gnss_date_time(record['fields']['Date'],\n",
      "                                                      record['fields']['Time'])\n",
      "            \n",
      "            # difference between the two clocks\n",
      "            delta_time = gnss_datetime - timestamp\n",
      "            previous_timestamp = timestamp\n",
      "            logging.info(f\"Found GNSS record at {gnss_datetime}\")\n",
      "            logging.info(f\"Delta time is {delta_time}.\")\n",
      "        elif (pgn in pgns_to_collect) and (delta_time is not None): \n",
      "            # Wait until we've seen at least one GNSS message\n",
      "            row = copy.copy(record['fields'])\n",
      "            timestamp = arrow.get(record['timestamp'])\n",
      "            # Had a weird special cases where the same timestamp appeared twice.  Let's just skip.\n",
      "            if (timestamp - previous_timestamp) > MIN_DELTA_TIME:\n",
      "                previous_timestamp = timestamp\n",
      "                row['src'] = src\n",
      "                row['timestamp'] = timestamp.datetime\n",
      "                row['row_times'] = (timestamp + delta_time).datetime\n",
      "                pgn_rows[pgn].append(row)\n",
      "    res = {}\n",
      "    for pgn, rows in pgn_rows.items():\n",
      "        df = pd.DataFrame(rows)\n",
      "        # Rename columns so that they make sense\n",
      "        res[pgn] = df.rename(columns=cb.canonical_field_name)\n",
      "    return res\n",
      "\n",
      "def json_to_ais_data_frame(json_file, max_rows=None):\n",
      "    \"Read the JSON log file and produce a dataframe contains all lat/lon fixes contained in the AIS logs.\"\n",
      "    records = cb.json_records( cb.file_lines(json_file), None, 1)\n",
      "    records = it.islice(records, 0, max_rows)\n",
      "\n",
      "    dfs = ais_dataframes(records)\n",
      "\n",
      "    df_pos = dfs[PGN_AIS_POSITION]\n",
      "    # Just grab the critical columns, and then remove dups\n",
      "    df_data = dfs[PGN_AIS_DATA][['user_id', 'name']].drop_duplicates()\n",
      "    # Join the two dataframes, adds the name column to the position data\n",
      "    df_joined = df_pos.merge(df_data, on='user_id')\n",
      "    # Sort by row_times, seems natural\n",
      "    return df_joined.sort_values(by=['row_times'])\n",
      "code\n",
      "##########################################################\n",
      "##########################################################\n",
      "##########################################################\n",
      "json_path = p.json_from_matching_named_file(\"2020-03-07\")\n",
      "# json_path = p.json_from_matching_named_file(\"2020-04-16\")\n",
      "\n",
      "df_all = json_to_ais_data_frame(json_path, None)  # 100000)\n",
      "bdf = df_all[df_all.name=='SONIC']\n",
      "code\n",
      "##########################################################\n",
      "##########################################################\n",
      "##########################################################\n",
      "display(Markdown(\"## Examine data for quality\"))\n",
      "\n",
      "ndf = bdf[\"user_id longitude latitude cog sog timestamp row_times\".split()]\n",
      "\n",
      "display(Markdown(\"### Every 20th row\"))\n",
      "display(ndf.iloc[::20])\n",
      "\n",
      "display(Markdown(\"### Data Frequency\"))\n",
      "display(ndf.row_times.diff().describe())\n",
      "\n",
      "display(Markdown(\"**There can be very large gaps.**\"))\n",
      "\n",
      "\n",
      "display(Markdown('### Some statistics'))\n",
      "display(ndf.describe())\n",
      "\n",
      "display(Markdown('Note that the max *longitude* and min *latitude* are wrong!'))\n",
      "\n",
      "\n",
      "display(Markdown('### Missing Data'))\n",
      "\n",
      "display(ndf.isna().sum())\n",
      "\n",
      "number_missing = ndf.isna().sum(1).sum()\n",
      "\n",
      "display(Markdown(f\"There are a total of {number_missing} NaN entries.\"))\n",
      "\n",
      "code\n",
      "##########################################################\n",
      "##########################################################\n",
      "##########################################################\n",
      "MAX_VALID_DISTANCE = 100000  # 100 kilometers \n",
      "\n",
      "def clean_ais_df(df):\n",
      "    \"\"\"\n",
      "    Test that the AIS data is valid.  (Not perfect!)\n",
      "\n",
      "    Sometimes the data extracted from AIS is corrupt.  Missing values (NaN) and corrupt values.\n",
      "    \"\"\"\n",
      "    df = df[df.isna().sum(1) == 0]\n",
      "    locations = np.vstack(G.MAP(np.asarray(df.longitude), np.asarray(df.latitude))).T\n",
      "    # We will reject distances that are too far from the center of our map.\n",
      "    is_valid = np.all(np.logical_and((locations < MAX_VALID_DISTANCE), (locations > -MAX_VALID_DISTANCE)), axis=1)\n",
      "    return df[is_valid]\n",
      "\n",
      "cdf = clean_ais_df(ndf)\n",
      "\n",
      "display(Markdown(\"## Much Cleaner Data\"))\n",
      "\n",
      "display(cdf.describe())\n",
      "\n",
      "display(cdf.isna().sum())\n",
      "\n",
      "number_missing = cdf.isna().sum(1).sum()\n",
      "\n",
      "display(Markdown(f\"There are a total of {number_missing} NaN entries.\"))\n",
      "code\n",
      "##########################################################\n",
      "##########################################################\n",
      "##########################################################\n",
      "ch = c.plot_chart(cdf)\n",
      "c.draw_track(cdf, ch, color='red', marker='.')markdown\n",
      "code\n",
      "##########################################################\n",
      "##########################################################\n",
      "##########################################################\n",
      "def fit_spline(df, plot=False):\n",
      "    basetime = df.row_times.min()\n",
      "    # Get the times, in seconds.\n",
      "    x = np.array((df.row_times - basetime) / pd.Timedelta('1s'))\n",
      "    # The dependent variable is position.  Use map position rather than lat/lon\n",
      "    y = np.vstack(G.MAP(np.asarray(df.longitude), np.asarray(df.latitude))).T\n",
      "    # Decompose the derivative into two components, north and east\n",
      "    vog_north = df.sog * p.cos_d(df.cog)\n",
      "    vog_east = df.sog * p.sin_d(df.cog)\n",
      "    # Combine into a single matrix\n",
      "    dy = np.vstack((vog_east, vog_north)).T\n",
      "\n",
      "    # Compute spline\n",
      "    cubic_spline = scipy.interpolate.CubicHermiteSpline(x, y, dy)\n",
      "\n",
      "    # Create a new set of spline points, every 5 seconds.\n",
      "    x_new = np.linspace(x.min(), x.max(), int((x.max() - x.min())/5))\n",
      "    loc_new = cubic_spline(x_new)\n",
      "    # Convert back to lat/lon\n",
      "    lon_new, lat_new = G.MAP(loc_new[:,0], loc_new[:,1], inverse=True)\n",
      "\n",
      "    # datetime conversions (painful)\n",
      "    datetime64 = np.datetime64(basetime.tz_convert('UTC').tz_localize(None))\n",
      "    time_new = datetime64 + x_new * pd.Timedelta('1s')\n",
      "\n",
      "    # New DataFrame\n",
      "    ndf = pd.DataFrame(dict(row_times=time_new, latitude = lat_new, longitude = lon_new))\n",
      "    ndf.row_times = ndf.row_times.dt.tz_localize('UTC').dt.tz_convert('US/Pacific')\n",
      "    # Current GPX needs an altitude, make one up!\n",
      "    ndf['altitude'] = 0\n",
      "    if plot:\n",
      "        ch = c.plot_chart(ndf)\n",
      "        c.draw_track(ndf, ch)\n",
      "        c.draw_track(df, ch, color='red', linestyle = 'None', marker='.')\n",
      "    return ndf\n",
      "\n",
      "sdf = fit_spline(cdf, True)markdown\n",
      "code\n",
      "##########################################################\n",
      "##########################################################\n",
      "##########################################################\n",
      "import importlib\n",
      "importlib.reload(ais_process)code\n",
      "##########################################################\n",
      "##########################################################\n",
      "##########################################################\n",
      "import ais_process\n",
      "\n",
      "json_path = p.json_from_matching_named_file(\"2020-04-16\")\n",
      "df_all = ais_process.json_to_ais_data_frame(json_path, None)  # 100000)\n",
      "code\n",
      "##########################################################\n",
      "##########################################################\n",
      "##########################################################\n",
      "bdf = df_all[df_all.name=='CREATIVE']\n",
      "ndf = bdf[\"name user_id longitude latitude cog sog timestamp row_times\".split()]\n",
      "cdf = ais_process.clean_ais_df(ndf)\n",
      "sdf = ais_process.fit_spline(cdf, True)\n",
      "code\n",
      "##########################################################\n",
      "##########################################################\n",
      "##########################################################\n",
      "ais_process.ais_df_to_gpx(sdf, \"/Users/viola/tmp/foo.gpx\")"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'notebook.services'; 'notebook' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-90bb7ccc7820>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnotebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnotebook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilemanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'notebook.services'; 'notebook' is not a package"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import notebook\n",
    "import notebook.services.contents.filemanager\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " 'cell',\n",
       " 'fs',\n",
       " 'json',\n",
       " 'line',\n",
       " 'nb',\n",
       " 'nb_file',\n",
       " 'source_code']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'notebook.services'; 'notebook' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d8b8dd6ddb06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnotebook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'notebook.services'; 'notebook' is not a package"
     ]
    }
   ],
   "source": [
    "import notebook.services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sail] *",
   "language": "python",
   "name": "conda-env-sail-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
